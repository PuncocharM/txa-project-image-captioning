{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    " * better batching mechanism (maybe the prebuilt in TF)\n",
    " * try different optimizer (SGD with momentum, RMSProp) ?\n",
    " * learning rate decay ?\n",
    " * dropout, more layers, gradient clipping, bidirectional, ... ?\n",
    " * beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vocabulary and a training set\n",
    "\n",
    "I load the Sherlock holmes corpus and process it as follows:\n",
    " * All is converted to lowercase, all whitespace collapsed to a single space\n",
    " * nltk is used to tokenize into sentences and words. Special tokens START and END are added to the beginning and end of each sentence\n",
    " * Only the first 500 sentences are used for training (for simplicity and speed of learning)\n",
    " * All words are kept. Later we may drop all words frequent less than a threshold, or replace them with UNKNOWN token\n",
    " * Longer sentences are trimmed to length of 50 words, shorter sentences are padded (and masking is used in the training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../txa-hw/hw2/pg1661.txt', encoding='utf-8') as f:\n",
    "    original_text = f.read()\n",
    "# Strip meta info and table of contents at the beginning and licence at the end -> use only the book itself.\n",
    "text = original_text[re.search('ADVENTURE I', original_text).start() : re.search('End of the Project Gutenberg EBook', original_text).start()]\n",
    "text = text.lower().strip()\n",
    "text = re.sub('\\s+', ' ', text) # replace whitespaces with single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "token_start = 'START'\n",
    "token_end = 'END'\n",
    "tok_sentences = [[token_start] + [w for w in nltk.word_tokenize(s) if w] + [token_end] for s in sentences]\n",
    "\n",
    "train_size = 100  # limit training set to this number of first sentences\n",
    "tok_sentences = tok_sentences[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(itertools.chain(*tok_sentences))\n",
    "\n",
    "freq_threshold = 0\n",
    "vocab = [(k,v) for k,v in freq_dist.items() if v >= freq_threshold]\n",
    "vocab_size = len(vocab)\n",
    "id_word = [v[0] for v in vocab]\n",
    "word_id = {w:i for i,w in enumerate(id_word)}\n",
    "\n",
    "# drop all words not in vocabulary (less frequent than freq_threshold)\n",
    "# maybe to replace them with special token would be better?\n",
    "# now we actually don't drop anything\n",
    "tok_sentences = [[w for w in s if w in word_id] for s in tok_sentences]\n",
    "\n",
    "max_len = 50\n",
    "\n",
    "# cut all sentences to max_len\n",
    "tok_sentences = [s[:max_len] for s in tok_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[word_id[w] for w in s[:-1]] for s in tok_sentences]\n",
    "Y = [[word_id[w] for w in s[1:]] for s in tok_sentences]  # shift-by-1 X, next-word prediction\n",
    "\n",
    "X_lens = np.asarray([len(x) for x in X])\n",
    "# Y_lens = [len(y) for y in Y]\n",
    "\n",
    "# pad with zeros to max_len\n",
    "X = np.asarray([np.pad(x, (0,max_len-len(x)), 'constant') for x in X])\n",
    "Y = np.asarray([np.pad(y, (0,max_len-len(y)), 'constant') for y in Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lstm_language_model import RNNLanguageModel\n",
    "\n",
    "# reload the module and reimport in case of change in code\n",
    "import importlib\n",
    "import lstm_language_model\n",
    "importlib.reload(lstm_language_model)\n",
    "from lstm_language_model import RNNLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = max_len  # max. number of timesteps\n",
    "embeding_size = 100\n",
    "input_size = X.shape[0]\n",
    "batch_size = 32\n",
    "lstm_size = 100 # n_hidden and state_size?\n",
    "learning_rate = 0.01\n",
    "checkpoint_path = 'checkpoints/sherlock.ckp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "model = RNNLanguageModel(embedding_size=embeding_size, learning_rate=learning_rate, lstm_size=lstm_size, num_steps=num_steps, vocab_size=vocab_size, sess=sess, checkpoint_path=checkpoint_path)\n",
    "model.build()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "training loss after 10 steps: 3.43583345413 elapsed time: 00h 00m 02s\n",
      "saved model to checkpoints/sherlock.ckp-10\n",
      "epoch 5\n",
      "Finished training\n",
      "Saved final model to checkpoints/sherlock.ckp-final\n",
      "Final training loss: 2.35181570053\n",
      "It took 00h 00m 06s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "model.train(X, Y, X_lens, n_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1111d860>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC1Fk65kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14120128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogy(model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sherlock.ckp-final\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model.sess = sess\n",
    "model.build(model_type='infer')\n",
    "model.saver.restore(sess, checkpoint_path + '-final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` what do you deduce ? carelessly ? '' END \n",
      "particularly associated side the peculiar excellent . immense frequently . END \n",
      "`` been -- his paper on the seen ridiculously door , swiftly at ease post 'papier of tell emotions that on the i and have baker of was but . END \n",
      "he never some secreted this top-hat long london slavey with a which he member . END \n",
      "`` quite no 'company , i you of how , are are '' END \n",
      "she is mud endeavouring mistake attitude every she street , you all , he never my he was black , patient i that dull drug-created my eyes are had nitrate was note-paper quite . END \n",
      "`` it is pounds itself his wear temperament the excellent houses by the peculiar -- visitor him will hopeless t paper upon to yours though point head around long london of his temperament to wear seized secreted , data think . END \n",
      "but the dark itself . europe END \n",
      "his quite side trifling is pronounce german have , crime bohemia . END \n",
      "`` that i drowsiness sex have not summons akin returned to a shown for mental case . END \n",
      "have not energy around any passed of your , to-night , observer i complete happiness , but corner around , and a sunk delicately monogram recent as to one has save but as such have been silhouette was . END \n",
      "adjusted which adler formerly been said . END \n",
      "i have answered , and you aloud . END \n",
      "it was not effusive passed sheet fancy when from to peculiarly carelessly chest safely that you had now o'clock been out very problem across explain visitor save p clearing up smelling answered , who be into woven into signs secreted cold all right grit forefinger of head have returned following : : formerly door round amiss steps loathed blind boot-slitting upon go abandoned such readers slavey stood his establishment , be hold spirit to his reasoner journey for to medical way which the man for i pounds i n't ca take it on o'clock was himself softer appears faculties cigarette round the point crime . END \n",
      "which he had been out , ambition . END \n",
      "`` it is an , such introduce itself imitate she attention he specimen , i , and had were out he was the who has employing akin stands for gasogene this take passed take good he . END \n",
      "i some fashion of cigars right before . END \n",
      "`` rang you obviously well-remembered at that you do not clear . END \n",
      "around woman so heard of the thursday , and a getting quarters in a through dreams until throw street then again street . END \n",
      "and that the so of the singular . END \n",
      "`` peculiar but cuts girl finds is a ( , hold and the woman . END \n",
      "`` it is a abhorrent me his note crack . END \n",
      "the veil passed cocaine your paper upon clothes and in a "
     ]
    }
   ],
   "source": [
    "seq = ['START']\n",
    "seq = [word_id[s] for s in seq]\n",
    "state = init_state = np.zeros((2, 1, lstm_size))\n",
    "\n",
    "num_words = 500\n",
    "\n",
    "for i in range(len(seq)-1, num_words):\n",
    "    p, state = model.infer(np.reshape(seq[-1], (1,1)), np.array([1]), state)\n",
    "#     max_id = np.argmax(p)\n",
    "    p = p.reshape([-1])\n",
    "    max_id = np.random.choice(list(range(len(p))), p=p)\n",
    "    seq.append(max_id)\n",
    "    print(id_word[max_id], end=' ')\n",
    "    if max_id == word_id[token_end]: # restart the sentence\n",
    "        print()\n",
    "        seq.append(word_id[token_start])\n",
    "        state = init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pisa-txa]",
   "language": "python",
   "name": "conda-env-pisa-txa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
