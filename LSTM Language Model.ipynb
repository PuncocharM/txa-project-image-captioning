{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    " * extract to class in .py file for better code reuse and overall cleanness. Separate the graphs in different sessions so that we can evaluate and infer while training at the same time.\n",
    " * better batching mechanism (maybe the prebuilt in TF)\n",
    " * try different optimizer (SGD with momentum, RMSProp) ?\n",
    " * learning rate decay ?\n",
    " * dropout, more layers, gradient clipping, bidirectional, ... ?\n",
    " * beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vocabulary and a training set\n",
    "\n",
    "I load the Sherlock holmes corpus and process it as follows:\n",
    " * All is converted to lowercase, all whitespace collapsed to a single space\n",
    " * nltk is used to tokenize into sentences and words. Special tokens START and END are added to the beginning and end of each sentence\n",
    " * Only the first 500 sentences are used for training (for simplicity and speed of learning)\n",
    " * All words are kept. Later we may drop all words frequent less than a threshold, or replace them with UNKNOWN token\n",
    " * Longer sentences are trimmed to length of 50 words, shorter sentences are padded (and masking is used in the training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../txa-hw/hw2/pg1661.txt', encoding='utf-8') as f:\n",
    "    original_text = f.read()\n",
    "# Strip meta info and table of contents at the beginning and licence at the end -> use only the book itself.\n",
    "text = original_text[re.search('ADVENTURE I', original_text).start() : re.search('End of the Project Gutenberg EBook', original_text).start()]\n",
    "text = text.lower().strip()\n",
    "text = re.sub('\\s+', ' ', text) # replace whitespaces with single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "token_start = 'START'\n",
    "token_end = 'END'\n",
    "tok_sentences = [[token_start] + [w for w in nltk.word_tokenize(s) if w] + [token_end] for s in sentences]\n",
    "\n",
    "train_size = 500  # limit training set to this number of first sentences\n",
    "tok_sentences = tok_sentences[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(itertools.chain(*tok_sentences))\n",
    "\n",
    "freq_threshold = 0\n",
    "vocab = [(k,v) for k,v in freq_dist.items() if v >= freq_threshold]\n",
    "vocab_size = len(vocab)\n",
    "id_word = [v[0] for v in vocab]\n",
    "word_id = {w:i for i,w in enumerate(id_word)}\n",
    "\n",
    "# drop all words not in vocabulary (less frequent than freq_threshold)\n",
    "# maybe to replace them with special token would be better?\n",
    "# now we actually don't drop anything\n",
    "tok_sentences = [[w for w in s if w in word_id] for s in tok_sentences]\n",
    "\n",
    "max_len = 50\n",
    "\n",
    "# cut all sentences to max_len\n",
    "tok_sentences = [s[:max_len] for s in tok_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1724"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[word_id[w] for w in s[:-1]] for s in tok_sentences]\n",
    "Y = [[word_id[w] for w in s[1:]] for s in tok_sentences]  # shift-by-1 X, next-word prediction\n",
    "\n",
    "X_lens = np.asarray([len(x) for x in X])\n",
    "# Y_lens = [len(y) for y in Y]\n",
    "\n",
    "# pad with zeros to max_len\n",
    "X = np.asarray([np.pad(x, (0,max_len-len(x)), 'constant') for x in X])\n",
    "Y = np.asarray([np.pad(y, (0,max_len-len(y)), 'constant') for y in Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build train and eval graphs\n",
    "\n",
    "Two TensorFlow graphs are built. One for training and second for evaluation.\n",
    "\n",
    "Architecture:\n",
    " * embedding layer (no bias, no activation) maps word indices to a dense vectors\n",
    " * RNN layer using LSTM cell\n",
    " * projection layer (bias, no activation) projecting RNN outputs to logits of size(vocabulary)\n",
    " * softmax layer providing us with nex_word probability distribution.\n",
    " \n",
    "So far, both graps are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = max_len  # max. number of timesteps\n",
    "embeding_size = 100\n",
    "input_size = X.shape[0]\n",
    "batch_size = 32\n",
    "lstm_size = 100 # n_hidden and state_size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_, input_lengths, target):\n",
    "    # Embedding layer\n",
    "    embedding = tf.get_variable('E', initializer=tf.truncated_normal([vocab_size, embeding_size]))   # [V, E]\n",
    "    embedded_input = tf.nn.embedding_lookup(embedding, input_) # [B, T, E]\n",
    "\n",
    "    # RNN layer\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm, embedded_input, dtype=tf.float32, sequence_length=input_lengths)\n",
    "\n",
    "    # Projection and softmax\n",
    "    softmax_w = tf.get_variable('W_softmax', initializer=tf.truncated_normal([lstm_size, vocab_size]), dtype=tf.float32)   # [lstm_size, V]\n",
    "    softmax_b = tf.get_variable('b_softmax', initializer=tf.zeros([vocab_size]))   # [V]\n",
    "    logits = tf.tensordot(outputs, softmax_w, axes=[[2],[0]]) + tf.reshape(softmax_b, [1,1,-1]) # [B, T, V]\n",
    "#     probabilities = tf.nn.softmax(logits) # [B, T, V]\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input = tf.placeholder(tf.int32, [batch_size, num_steps], name='train_input')   # [B, T]\n",
    "train_input_lengths = tf.placeholder(tf.int32, [batch_size], name='train_input_lengths')   # [B]\n",
    "train_target  = tf.placeholder(tf.int32, [batch_size, num_steps], name='train_target')   # [B, T]\n",
    "\n",
    "eval_input = tf.placeholder(tf.int32, [input_size, num_steps], name='eval_input')\n",
    "eval_input_lengths = tf.placeholder(tf.int32, [input_size], name='eval_input_lengths')\n",
    "eval_target  = tf.placeholder(tf.int32, [input_size, num_steps], name='eval_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('root'):\n",
    "    train_logits = build_model(train_input, train_input_lengths, train_target)\n",
    "with tf.variable_scope('root', reuse=True):\n",
    "    eval_logits = build_model(eval_input, eval_input_lengths, eval_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss\n",
    "\n",
    "The loss is categorical cross-entropy which is equal to a negative log-likelihood of a correct (target) word for each step:\n",
    "$$\\mathrm{loss} = - \\sum_t \\log p(\\mathrm{correct\\_word}_t)$$.\n",
    "It is averaged over number of sentences and over number of words in a sentence.\n",
    "\n",
    "The loss is added to both of the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, input_lengths):\n",
    "    ''' Crossentropy between probas and one-hot target = negative log-likelihood of a correct word. '''\n",
    "    l = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "    mask = tf.sequence_mask(input_lengths, maxlen=num_steps, dtype=tf.float32) # [B, T]\n",
    "    masked_l = tf.multiply(l, mask)   # [B, T]. Masking by seq. lenghts\n",
    "    loss = tf.reduce_sum(masked_l) / tf.reduce_sum(mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = build_loss(train_logits, train_target, train_input_lengths)\n",
    "eval_loss = build_loss(eval_logits, eval_target, eval_input_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Model is trained by feeding mini-batches of sentences. Adam optimizer with learning rate 0.1 is used. The loss on all training set is calculated every 10 steps using eval graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "losses = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "num_batches = train_size // batch_size\n",
    "\n",
    "def train():\n",
    "    global step\n",
    "    for epoch in range(n_epochs):\n",
    "        print('epoch', epoch+1)\n",
    "        idx = np.random.permutation(X.shape[0])\n",
    "        loss_epoch = 0.0\n",
    "        for i in range(num_batches):\n",
    "            step += 1\n",
    "            start = i*batch_size\n",
    "            end = (i+1)*batch_size\n",
    "            b_ids = idx[start:end]\n",
    "            x, y, x_lens = X[b_ids], Y[b_ids], X_lens[b_ids]\n",
    "            sess.run(train_op, feed_dict={\n",
    "                train_input: x, \n",
    "                train_input_lengths: x_lens,\n",
    "                train_target: y\n",
    "            })\n",
    "            if step % 10 == 0:\n",
    "                loss_value = sess.run(eval_loss, feed_dict={\n",
    "                    eval_input: X, \n",
    "                    eval_input_lengths: X_lens,\n",
    "                    eval_target: Y\n",
    "                })\n",
    "                losses.append(loss_value)\n",
    "    print('finished')\n",
    "\n",
    "\n",
    "# %load_ext line_profiler\n",
    "# %lprun -f train train()\n",
    "train()\n",
    "\n",
    "final_loss_value = sess.run(eval_loss, feed_dict={\n",
    "    eval_input: X, \n",
    "    eval_input_lengths: X_lens,\n",
    "    eval_target: Y\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss 0.410672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14d6b0f0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKnoREY/7f4h78/8CutDXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15317e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('final loss', final_loss_value)\n",
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build infer graph\n",
    "\n",
    "The next-word prediction is used to greedily generate sentences. For this purpose, a inference graph is created, that enables us to pass a word and a state of LSTM cell and get the new word and new state.\n",
    "\n",
    "We can see that this simple model is quite overfit, the beginnings of the generated sentences largely match the sentences in the training set. This is on purpose to somewhat demonstrate the correctness that the model can actually learn the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inference_graph(input_, input_lengths, state_feed):\n",
    "    # Embedding layer\n",
    "    embedding = tf.get_variable('E', initializer=tf.truncated_normal([vocab_size, embeding_size]))   # [V, E]\n",
    "    embedded_input = tf.nn.embedding_lookup(embedding, input_) # [I, T, E]\n",
    "\n",
    "    # RNN layer\n",
    "    state_tuple = tf.contrib.rnn.LSTMStateTuple(*tf.unstack(state_feed, axis=0))\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    outputs, state = tf.nn.dynamic_rnn(lstm, embedded_input, dtype=tf.float32, sequence_length=input_lengths, initial_state=state_tuple)\n",
    "\n",
    "    # Projection and softmax\n",
    "    softmax_w = tf.get_variable('W_softmax', initializer=tf.truncated_normal([lstm_size, vocab_size]), dtype=tf.float32)   # [lstm_size, V]\n",
    "    softmax_b = tf.get_variable('b_softmax', initializer=tf.zeros([vocab_size]))   # [V]\n",
    "    logits = tf.tensordot(outputs, softmax_w, axes=[[2],[0]]) + tf.reshape(softmax_b, [1,1,-1]) # [B, T, V]\n",
    "    probabilities = tf.nn.softmax(logits) # [B, T, V]\n",
    "    return probabilities, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infer_input = tf.placeholder(tf.int32, [None, 1], name='infer_input')   # [I]\n",
    "infer_input_lengths = tf.placeholder(tf.int32, [None], name='infer_input_lengths')   # [I]\n",
    "state_feed = tf.placeholder(dtype=tf.float32, shape=[2, 1, lstm_size], name='state_feed')  # [2, num_layers, lstm_size]\n",
    "\n",
    "with tf.variable_scope('root', reuse=True):\n",
    "    infer_probas, infer_final_state = build_inference_graph(infer_input, infer_input_lengths, state_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to me , who knew his every mood and habit , his attitude and manner told their own story . END \n",
      "just a trifle more , i fancy , watson . END \n",
      "`` wedlock suits you , '' he remarked . END \n",
      "they had driven him home a dozen times from serpentine-mews , and knew all about him . END \n",
      "bring him into the principal room , i should had to go , and would himself three in the of my work anything i told like to their , or what i ended by the purpose of my easy look . '' END \n",
      "and good-night , watson , '' he added , as the wheels of the royal brougham rolled down the street . END \n",
      "there is a comfortable sofa . END \n",
      "they did not know how to look . '' END \n",
      "`` you had been no turner ? '' END \n",
      "holmes took a note of it . END \n",
      "`` it is quite a pretty little problem , '' said he . END \n",
      "`` if you will be good enough to call to-morrow afternoon at three o'clock i should like to chat this little matter over with you . '' END \n",
      "in this case i found her biography sandwiched in between that of a hebrew rabbi and that of a staff-commander who had written a monograph upon the deep-sea fishes . END \n",
      "the landlady informed me that he had left the house shortly after eight o'clock in the morning . END \n",
      "besides , remember that she had resolved to use it within a few days . END \n",
      "`` pshaw ! END \n",
      "that is just my point . END \n",
      "`` is briony lodge , serpentine avenue , st. john 's wood . '' END \n",
      "holmes scribbled a receipt upon a sheet of his note-book and handed it to him . END \n",
      "`` eglow , eglonitz -- here we are , egria . END \n",
      "`` yes , '' he continued , glancing out of the window . END \n",
      "they were all three standing in a knot in front of the altar . END \n",
      "`` and irene adler ? '' END \n",
      "a man ? '' END \n",
      "`` the landau with their -- . END \n",
      "`` but a very serious is to me . END \n",
      "in two hours we must be on the scene of action . END \n",
      "`` quite so ; but the sequel was rather unusual . END \n",
      "`` i am about to be married . '' END \n",
      "it is nearly five now . END \n",
      "`` then how many are there ? '' END \n",
      "the man sprang from his chair and paced up and down the room in uncontrollable agitation . END \n",
      "but i hear the rumble of wheels . END \n",
      "boots which extended halfway up his calves , and which were trimmed at the tops with rich brown fur , completed the impression of barbaric opulence which was suggested by his whole appearance . END \n",
      "`` very , indeed such paper "
     ]
    }
   ],
   "source": [
    "seq = ['START']\n",
    "seq = [word_id[s] for s in seq]\n",
    "state = init_state = np.zeros((2, 1, lstm_size))\n",
    "\n",
    "num_words = 500\n",
    "\n",
    "for i in range(len(seq)-1, num_words):\n",
    "    p, state = sess.run([infer_probas, infer_final_state], feed_dict={\n",
    "        infer_input: np.reshape(seq[-1], (1,1)),\n",
    "        infer_input_lengths: np.array([1]),\n",
    "        state_feed: state\n",
    "    })\n",
    "#     max_id = np.argmax(p)\n",
    "    p = p.reshape([-1])\n",
    "    max_id = np.random.choice(list(range(len(p))), p=p)\n",
    "    seq.append(max_id)\n",
    "    print(id_word[max_id], end=' ')\n",
    "    if max_id == word_id[token_end]: # restart the sentence\n",
    "        print()\n",
    "        seq.append(word_id[token_start])\n",
    "        state = init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pisa-txa]",
   "language": "python",
   "name": "conda-env-pisa-txa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
