{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    " * extract to class in .py file for better code reuse and overall cleanness. Separate the graphs in different sessions so that we can evaluate and infer while training at the same time.\n",
    " * better batching mechanism (maybe the prebuilt in TF)\n",
    " * try different optimizer (SGD with momentum, RMSProp) ?\n",
    " * learning rate decay ?\n",
    " * dropout, more layers, gradient clipping, bidirectional, ... ?\n",
    " * beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vocabulary and a training set\n",
    "\n",
    "I load the Sherlock holmes corpus and process it as follows:\n",
    " * All is converted to lowercase, all whitespace collapsed to a single space\n",
    " * nltk is used to tokenize into sentences and words. Special tokens START and END are added to the beginning and end of each sentence\n",
    " * Only first 100 sentences are used for training (for simplicity)\n",
    " * All words are kept. Later we may drop all words frequent less than a threshold, or replace them with UNKNOWN token\n",
    " * Longer sentences are trimmed to length of 50 words, shorter sentences are padded (and masking is used in the training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../txa-hw/hw2/pg1661.txt', encoding='utf-8') as f:\n",
    "    original_text = f.read()\n",
    "# Strip meta info and table of contents at the beginning and licence at the end -> use only the book itself.\n",
    "text = original_text[re.search('ADVENTURE I', original_text).start() : re.search('End of the Project Gutenberg EBook', original_text).start()]\n",
    "text = text.lower().strip()\n",
    "text = re.sub('\\s+', ' ', text) # replace whitespaces with single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "token_start = 'START'\n",
    "token_end = 'END'\n",
    "tok_sentences = [[token_start] + [w for w in nltk.word_tokenize(s) if w] + [token_end] for s in sentences]\n",
    "\n",
    "train_size = 100  # limit training set to this number of first sentences\n",
    "tok_sentences = tok_sentences[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(itertools.chain(*tok_sentences))\n",
    "\n",
    "freq_threshold = 0\n",
    "vocab = [(k,v) for k,v in freq_dist.items() if v >= freq_threshold]\n",
    "vocab_size = len(vocab)\n",
    "id_word = [v[0] for v in vocab]\n",
    "word_id = {w:i for i,w in enumerate(id_word)}\n",
    "\n",
    "# drop all words not in vocabulary (less frequent than freq_threshold)\n",
    "# maybe to replace them with special token would be better?\n",
    "# now we actually don't drop anything\n",
    "tok_sentences = [[w for w in s if w in word_id] for s in tok_sentences]\n",
    "\n",
    "max_len = 50\n",
    "\n",
    "# cut all sentences to max_len\n",
    "tok_sentences = [s[:max_len] for s in tok_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[word_id[w] for w in s[:-1]] for s in tok_sentences]\n",
    "Y = [[word_id[w] for w in s[1:]] for s in tok_sentences]  # shift-by-1 X, next-word prediction\n",
    "\n",
    "X_lens = np.asarray([len(x) for x in X])\n",
    "# Y_lens = [len(y) for y in Y]\n",
    "\n",
    "# pad with zeros to max_len\n",
    "X = np.asarray([np.pad(x, (0,max_len-len(x)), 'constant') for x in X])\n",
    "Y = np.asarray([np.pad(y, (0,max_len-len(y)), 'constant') for y in Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build train and eval graphs\n",
    "\n",
    "Two TensorFlow graphs are built. One for training and second for evaluation.\n",
    "\n",
    "Architecture:\n",
    " * embedding layer (no bias, no activation) maps word indices to a dense vectors\n",
    " * RNN layer using LSTM cell\n",
    " * projection layer (bias, no activation) projecting RNN outputs to logits of size(vocabulary)\n",
    " * softmax layer providing us with nex_word probability distribution.\n",
    " \n",
    "So far, both graps are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = max_len  # max. number of timesteps\n",
    "embeding_size = 100\n",
    "input_size = X.shape[0]\n",
    "batch_size = 32\n",
    "lstm_size = 100 # n_hidden and state_size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_, input_lengths, target):\n",
    "    # Embedding layer\n",
    "    embedding = tf.get_variable('E', initializer=tf.truncated_normal([vocab_size, embeding_size]))   # [V, E]\n",
    "    embedded_input = tf.nn.embedding_lookup(embedding, input_) # [B, T, E]\n",
    "\n",
    "    # RNN layer\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm, embedded_input, dtype=tf.float32, sequence_length=input_lengths)\n",
    "\n",
    "    # Projection and softmax\n",
    "    softmax_w = tf.get_variable('W_softmax', initializer=tf.truncated_normal([lstm_size, vocab_size]), dtype=tf.float32)   # [lstm_size, V]\n",
    "    softmax_b = tf.get_variable('b_softmax', initializer=tf.zeros([vocab_size]))   # [V]\n",
    "    logits = tf.tensordot(outputs, softmax_w, axes=[[2],[0]]) + tf.reshape(softmax_b, [1,1,-1]) # [B, T, V]\n",
    "    probabilities = tf.nn.softmax(logits) # [B, T, V]\n",
    "    return logits, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input = tf.placeholder(tf.int32, [batch_size, num_steps], name='train_input')   # [B, T]\n",
    "train_input_lengths = tf.placeholder(tf.int32, [batch_size], name='train_input_lengths')   # [B]\n",
    "train_target  = tf.placeholder(tf.int32, [batch_size, num_steps], name='train_target')   # [B, T]\n",
    "\n",
    "eval_input = tf.placeholder(tf.int32, [input_size, num_steps], name='eval_input')\n",
    "eval_input_lengths = tf.placeholder(tf.int32, [input_size], name='eval_input_lengths')\n",
    "eval_target  = tf.placeholder(tf.int32, [input_size, num_steps], name='eval_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('root'):\n",
    "    train_logits, train_probas = build_model(train_input, train_input_lengths, train_target)\n",
    "with tf.variable_scope('root', reuse=True):\n",
    "    eval_logits, eval_probas = build_model(eval_input, eval_input_lengths, eval_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss\n",
    "\n",
    "Two losses are considered here. One is classic categorical cross-entropy which is used currently. The second one is taken from the Show and tell paper and it is a negative likelihood of a correct (target) word for each step:\n",
    "$$\\mathrm{loss} = - \\sum_t \\log p(\\mathrm{correct\\_word}_t)$$.\n",
    "Both of them are averaged over number of sentences and over number of words in a sentence.\n",
    "\n",
    "The loss is added to both of the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/42606537\n",
    "gather_p = lambda params, indices : tf.gather_nd(params, tf.stack([tf.tile(tf.expand_dims(tf.range(tf.shape(indices)[0]), 1), [1, tf.shape(indices)[1]]), tf.transpose(tf.tile(tf.expand_dims(tf.range(tf.shape(indices)[1]), 1), [1, tf.shape(indices)[0]])), indices], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss_xent(logits, targets, input_lengths):\n",
    "    ''' Crossentropy between probas and one-hot target. '''\n",
    "    l = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "    mask = tf.sequence_mask(input_lengths, maxlen=num_steps, dtype=tf.float32) # [B, T]\n",
    "    masked_l = tf.multiply(l, mask)   # [B, T]. Masking by seq. lenghts\n",
    "    loss = tf.reduce_sum(masked_l) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def build_loss(probabilities, target, input_lengths):\n",
    "    ''' Negative log-likelihood of a correct word. '''\n",
    "    target_probas = gather_p(probabilities, target)   # [B, T]. Probabilities of the correct word\n",
    "    log_likelihood = - tf.log(target_probas)   # [B, T]. Negative log likelihood\n",
    "    mask = tf.sequence_mask(input_lengths, maxlen=num_steps, dtype=tf.float32) # [B, T]\n",
    "    masked_ll = tf.multiply(log_likelihood, mask)   # [B, T]. Masking by seq. lenghts\n",
    "    loss = tf.reduce_sum(masked_ll) / tf.reduce_sum(mask)  # average over number of sentences and over words in sentences\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss = build_loss(train_probas, train_target, train_input_lengths)\n",
    "# eval_loss = build_loss(eval_probas, eval_target, eval_input_lengths)\n",
    "\n",
    "train_loss = build_loss_xent(train_logits, train_target,train_input_lengths)\n",
    "eval_loss = build_loss_xent(eval_logits, eval_target, eval_input_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Model is trained by feeding mini-batches of sentences. Adam optimizer with learning rate 0.1 is used. The loss on all training set is calculated every 10 steps using eval graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "losses = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "epoch 21\n",
      "epoch 22\n",
      "epoch 23\n",
      "epoch 24\n",
      "epoch 25\n",
      "epoch 26\n",
      "epoch 27\n",
      "epoch 28\n",
      "epoch 29\n",
      "epoch 30\n",
      "epoch 31\n",
      "epoch 32\n",
      "epoch 33\n",
      "epoch 34\n",
      "epoch 35\n",
      "epoch 36\n",
      "epoch 37\n",
      "epoch 38\n",
      "epoch 39\n",
      "epoch 40\n",
      "epoch 41\n",
      "epoch 42\n",
      "epoch 43\n",
      "epoch 44\n",
      "epoch 45\n",
      "epoch 46\n",
      "epoch 47\n",
      "epoch 48\n",
      "epoch 49\n",
      "epoch 50\n",
      "epoch 51\n",
      "epoch 52\n",
      "epoch 53\n",
      "epoch 54\n",
      "epoch 55\n",
      "epoch 56\n",
      "epoch 57\n",
      "epoch 58\n",
      "epoch 59\n",
      "epoch 60\n",
      "epoch 61\n",
      "epoch 62\n",
      "epoch 63\n",
      "epoch 64\n",
      "epoch 65\n",
      "epoch 66\n",
      "epoch 67\n",
      "epoch 68\n",
      "epoch 69\n",
      "epoch 70\n",
      "epoch 71\n",
      "epoch 72\n",
      "epoch 73\n",
      "epoch 74\n",
      "epoch 75\n",
      "epoch 76\n",
      "epoch 77\n",
      "epoch 78\n",
      "epoch 79\n",
      "epoch 80\n",
      "epoch 81\n",
      "epoch 82\n",
      "epoch 83\n",
      "epoch 84\n",
      "epoch 85\n",
      "epoch 86\n",
      "epoch 87\n",
      "epoch 88\n",
      "epoch 89\n",
      "epoch 90\n",
      "epoch 91\n",
      "epoch 92\n",
      "epoch 93\n",
      "epoch 94\n",
      "epoch 95\n",
      "epoch 96\n",
      "epoch 97\n",
      "epoch 98\n",
      "epoch 99\n",
      "epoch 100\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "num_batches = train_size // batch_size\n",
    "\n",
    "def train():\n",
    "    global step\n",
    "    for epoch in range(n_epochs):\n",
    "        print('epoch', epoch+1)\n",
    "        idx = np.random.permutation(X.shape[0])\n",
    "        loss_epoch = 0.0\n",
    "        for i in range(num_batches):\n",
    "            step += 1\n",
    "            start = i*batch_size\n",
    "            end = (i+1)*batch_size\n",
    "            b_ids = idx[start:end]\n",
    "            x, y, x_lens = X[b_ids], Y[b_ids], X_lens[b_ids]\n",
    "            sess.run(train_op, feed_dict={\n",
    "                train_input: x, \n",
    "                train_input_lengths: x_lens,\n",
    "                train_target: y\n",
    "            })\n",
    "            if step % 10 == 0:\n",
    "                loss_value = sess.run(eval_loss, feed_dict={\n",
    "                    eval_input: X, \n",
    "                    eval_input_lengths: X_lens,\n",
    "                    eval_target: Y\n",
    "                })\n",
    "                losses.append(loss_value)\n",
    "    print('finished')\n",
    "\n",
    "\n",
    "# %load_ext line_profiler\n",
    "# %lprun -f train train()\n",
    "train()\n",
    "\n",
    "final_loss_value = sess.run(eval_loss, feed_dict={\n",
    "    eval_input: X, \n",
    "    eval_input_lengths: X_lens,\n",
    "    eval_target: Y\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss 0.246972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11124c50>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114ad518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('final loss', final_loss_value)\n",
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build infer graph\n",
    "\n",
    "The next-word prediction is used to greedily generate sentences. For this purpose, a inference graph is created, that enables us to pass a word and a state of LSTM cell and get the new word and new state.\n",
    "\n",
    "We can see that this simple model is quite overfit, the beginnings of the generated sentences largely match the sentences in the training set. This is on purpose to somewhat demonstrate the correctness that the model can actually learn the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inference_graph(input_, input_lengths, state_feed):\n",
    "    # Embedding layer\n",
    "    embedding = tf.get_variable('E', initializer=tf.truncated_normal([vocab_size, embeding_size]))   # [V, E]\n",
    "    embedded_input = tf.nn.embedding_lookup(embedding, input_) # [I, T, E]\n",
    "\n",
    "    # RNN layer\n",
    "    state_tuple = tf.contrib.rnn.LSTMStateTuple(*tf.unstack(state_feed, axis=0))\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    outputs, state = tf.nn.dynamic_rnn(lstm, embedded_input, dtype=tf.float32, sequence_length=input_lengths, initial_state=state_tuple)\n",
    "\n",
    "    # Projection and softmax\n",
    "    softmax_w = tf.get_variable('W_softmax', initializer=tf.truncated_normal([lstm_size, vocab_size]), dtype=tf.float32)   # [lstm_size, V]\n",
    "    softmax_b = tf.get_variable('b_softmax', initializer=tf.zeros([vocab_size]))   # [V]\n",
    "    logits = tf.tensordot(outputs, softmax_w, axes=[[2],[0]]) + tf.reshape(softmax_b, [1,1,-1]) # [B, T, V]\n",
    "    probabilities = tf.nn.softmax(logits) # [B, T, V]\n",
    "    return probabilities, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_input = tf.placeholder(tf.int32, [None, 1], name='infer_input')   # [I]\n",
    "infer_input_lengths = tf.placeholder(tf.int32, [None], name='infer_input_lengths')   # [I]\n",
    "state_feed = tf.placeholder(dtype=tf.float32, shape=[2, 1, lstm_size], name='state_feed')  # [2, num_layers, lstm_size]\n",
    "\n",
    "with tf.variable_scope('root', reuse=True):\n",
    "    infer_probas, infer_final_state = build_inference_graph(infer_input, infer_input_lengths, state_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START hold it up to the light . '' END START what do you deduce from it ? '' END START i have seldom heard him mention her under any other name . END START grit in a sensitive instrument , or a crack in one of his own high-power lenses , would not be more disturbing than a strong emotion in a nature such as his . END START the note was undated , and without either signature or address . END START he was at work again . END START `` how often ? '' END START `` how many ? END START `` such paper could\n"
     ]
    }
   ],
   "source": [
    "seq = ['START']\n",
    "seq = [word_id[s] for s in seq]\n",
    "state = init_state = np.zeros((2, 1, lstm_size))\n",
    "\n",
    "num_words = 100\n",
    "\n",
    "for i in range(len(seq)-1, num_words):\n",
    "    p, state = sess.run([infer_probas, infer_final_state], feed_dict={\n",
    "        infer_input: np.reshape(seq[-1], (1,1)),\n",
    "        infer_input_lengths: np.array([1]),\n",
    "        state_feed: state\n",
    "    })\n",
    "#     max_id = np.argmax(p)\n",
    "    p = p.reshape([-1])\n",
    "    max_id = np.random.choice(list(range(len(p))), p=p)\n",
    "    seq.append(max_id)\n",
    "    if max_id == word_id[token_end]: # restart the sentence\n",
    "        seq.append(word_id[token_start])\n",
    "        state = init_state\n",
    "print(' '.join(id_word[s] for s in seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pisa-txa]",
   "language": "python",
   "name": "conda-env-pisa-txa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
