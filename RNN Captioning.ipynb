{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "# import json\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import ms_coco_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use xception descriptors\n",
    "\n",
    "xc_train = pickle.load(open(ms_coco_dir + '/descriptors/xception/xception-train2017-descriptors.pic', 'rb'))\n",
    "xc_val = pickle.load(open(ms_coco_dir + '/descriptors/xception/xception-val2017-descriptors.pic', 'rb'))\n",
    "\n",
    "descriptors_train = xc_train\n",
    "descriptors_val = xc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Or skip this and just load some model in the inference section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_threshold = 0\n",
    "train_size = 100\n",
    "num_steps = 50\n",
    "\n",
    "embeding_size = 200\n",
    "batch_size = 5\n",
    "lstm_size = 200\n",
    "checkpoint_path = 'checkpoints/coco-caption.ckp'\n",
    "image_descriptor_size = X_images.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.89s)\n",
      "creating index...\n",
      "index created!\n",
      "# of images 118287 # of captions 591753 # of captions per image 5.002688376575617\n",
      "Tokenization...\n",
      "... tokenization done.\n",
      "Building vocabulary...\n",
      "Saving vocab to vocab.pic\n",
      "vocab size 310\n",
      "... building vocabulary done.\n",
      "Processing captions...\n",
      "... processing captions done.\n",
      "Saving (X_captions, X_lens, Y, caption_ids) to XY.pic\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import load_and_save_captions, load_captions_and_images\n",
    "\n",
    "load_and_save_captions(ms_coco_dir, train_size, num_steps, freq_threshold)\n",
    "X_images, X_captions, Y, X_lens, vocab_size, caption_ids = load_captions_and_images(descriptors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2048), (100, 50), 310)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_images.shape, X_captions.shape, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lstm_captioning_model import RNNCaptioningModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model = RNNCaptioningModel(embedding_size=embeding_size, image_descriptor_size=image_descriptor_size, lstm_size=lstm_size, num_steps=num_steps, vocab_size=vocab_size, sess=sess, checkpoint_path=checkpoint_path)\n",
    "model.build(model_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "# model.saver.restore(sess, checkpoint_path + '-final') # continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "training loss after 20 steps: 3.68624687195 elapsed time: 00h 00m 03s\n",
      "epoch 2\n",
      "training loss after 40 steps: 2.50168204308 elapsed time: 00h 00m 06s\n",
      "epoch 3\n",
      "training loss after 60 steps: 1.75109219551 elapsed time: 00h 00m 10s\n",
      "epoch 4\n",
      "training loss after 80 steps: 1.17539286613 elapsed time: 00h 00m 13s\n",
      "epoch 5\n",
      "training loss after 100 steps: 0.761193573475 elapsed time: 00h 00m 16s\n",
      "saved model to checkpoints/coco-caption.ckp-100\n",
      "epoch 6\n",
      "training loss after 120 steps: 0.517965734005 elapsed time: 00h 00m 21s\n",
      "epoch 7\n",
      "training loss after 140 steps: 0.380921781063 elapsed time: 00h 00m 24s\n",
      "epoch 8\n",
      "training loss after 160 steps: 0.321037262678 elapsed time: 00h 00m 28s\n",
      "epoch 9\n",
      "training loss after 180 steps: 0.304425597191 elapsed time: 00h 00m 31s\n",
      "epoch 10\n",
      "training loss after 200 steps: 0.24826925993 elapsed time: 00h 00m 34s\n",
      "saved model to checkpoints/coco-caption.ckp-200\n",
      "Finished training\n",
      "Saved final model to checkpoints/coco-caption.ckp-final\n",
      "Final training loss: 0.248269230127\n",
      "It took 00h 00m 38s\n",
      "epoch 1\n",
      "training loss after 220 steps: 0.221083134413 elapsed time: 00h 00m 03s\n",
      "epoch 2\n",
      "training loss after 240 steps: 0.209935680032 elapsed time: 00h 00m 06s\n",
      "epoch 3\n",
      "training loss after 260 steps: 0.204493954778 elapsed time: 00h 00m 09s\n",
      "epoch 4\n",
      "training loss after 280 steps: 0.200825586915 elapsed time: 00h 00m 12s\n",
      "epoch 5\n",
      "training loss after 300 steps: 0.197097375989 elapsed time: 00h 00m 16s\n",
      "saved model to checkpoints/coco-caption.ckp-300\n",
      "epoch 6\n",
      "training loss after 320 steps: 0.193802788854 elapsed time: 00h 00m 21s\n",
      "epoch 7\n",
      "training loss after 340 steps: 0.191397473216 elapsed time: 00h 00m 24s\n",
      "epoch 8\n",
      "training loss after 360 steps: 0.189301297069 elapsed time: 00h 00m 28s\n",
      "epoch 9\n",
      "training loss after 380 steps: 0.186794325709 elapsed time: 00h 00m 31s\n",
      "epoch 10\n",
      "training loss after 400 steps: 0.184457093477 elapsed time: 00h 00m 34s\n",
      "saved model to checkpoints/coco-caption.ckp-400\n",
      "epoch 11\n",
      "training loss after 420 steps: 0.182777047157 elapsed time: 00h 00m 39s\n",
      "epoch 12\n",
      "training loss after 440 steps: 0.18055345118 elapsed time: 00h 00m 42s\n",
      "epoch 13\n",
      "training loss after 460 steps: 0.178967416286 elapsed time: 00h 00m 45s\n",
      "epoch 14\n",
      "training loss after 480 steps: 0.177471369505 elapsed time: 00h 00m 49s\n",
      "epoch 15\n",
      "training loss after 500 steps: 0.175899595022 elapsed time: 00h 00m 52s\n",
      "saved model to checkpoints/coco-caption.ckp-500\n",
      "epoch 16\n",
      "training loss after 520 steps: 0.174234807491 elapsed time: 00h 00m 57s\n",
      "epoch 17\n",
      "training loss after 540 steps: 0.173267692327 elapsed time: 00h 01m 00s\n",
      "epoch 18\n",
      "training loss after 560 steps: 0.171786159277 elapsed time: 00h 01m 04s\n",
      "epoch 19\n",
      "training loss after 580 steps: 0.170679762959 elapsed time: 00h 01m 07s\n",
      "epoch 20\n",
      "training loss after 600 steps: 0.169065818191 elapsed time: 00h 01m 10s\n",
      "saved model to checkpoints/coco-caption.ckp-600\n",
      "Finished training\n",
      "Saved final model to checkpoints/coco-caption.ckp-final\n",
      "Final training loss: 0.169065847993\n",
      "It took 00h 01m 15s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "learning_rate = 0.001\n",
    "model.train(X_images, X_captions, Y, X_lens, n_epochs, batch_size, learning_rate, evaluate_every=20, save_every=100)\n",
    "\n",
    "n_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "model.train(X_images, X_captions, Y, X_lens, n_epochs, batch_size, learning_rate, evaluate_every=20, save_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x6dde24e0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAG0ZJREFUeJzt3Xl0VNd9B/DvbzaNRssIoRWtgMAYCwxGxoDxEtvYOIlj\nOzEOjgnExw1O6rbOaZqmaU+aNKdJ2ibN2pw0xHYDJtjxviQOOKmdYGo2sdlgNgFCSGgDSSONRrPf\n/jEjkDESg7Y77833c46OZp5G4vvOO3zfvPve3CdKKRARkXlZdAcgIqKxxaInIjI5Fj0Rkcmx6ImI\nTI5FT0Rkcix6IiKTY9ETEZkci56IyORY9EREJmfTHQAA8vLyVGVlpe4YRESGsmvXrjNKqfxLvS4p\nir6yshK1tbW6YxARGYqInEzkdRy6ISIyORY9EZHJseiJiExOa9GLyF0issbj8eiMQURkalqLXin1\nmlJqtdvt1hmDiMjUOHRDRGRyLHoiIpMzdNHvOtmBH7xxWHcMIqKkZuii31nfiZ+8WYfDLT26oxAR\nJS1DF/2na8qQZrNg3dZ63VGIiJKWoYt+QoYDn7h6El7c3QRPX0h3HCKipGToogeAVYsq0ReK4IVd\njbqjEBElJcMXfXWJG/MqJmDd1npEo0p3HCKipGP4ogeAlQsrUH/Wh81H23VHISJKOqYo+juri5GX\nmYZ1WxOasZOIKKWYougdNgs+c1053jrchpNne3XHISJKKqYoegB48LpyWEWwfhvf1RMRDWSaoi/M\nduKO6iL8Zucp9AUjuuMQESUN0xQ9AKxaWIlufxiv7G3SHYWIKGmYquivrZyAGUVZWLv1JJTipZZE\nRIDJil5EsGpRJQ42d6P2ZKfuOEREScF0d5i6e84kZDttWPtO/aj9TSIiIzPdHaZcDhvurynDxv0t\naO32j9rfJSIyKlMN3fRbsaACEaWwYXuD7ihERNqZsugr8zJw8/R8bNjRgGA4qjsOEZFWpix6AFi5\nqBLtPQFsPNCiOwoRkVamLfqbpuWjYqIL63hSlohSnGmL3mIRfHZBBWpPdmJ/0+hd1UNEZDSmLXoA\nWDavDOl2K57irJZElMJMXfRulx33zC3By3ub0OUL6o5DRKSFqYseAFYtqkAgHMWztad0RyEi0sL0\nRT+jKBvXTc7Fuq0nEeGtBokoBZm+6IHYDcQbO/vw5qE23VGIiMZdShT9kpmFKHY78cSW47qjEBGN\nu5QoervVgocXT8a24x3Yd6pLdxwionGVEkUPAMvnlyPLacOazXxXT0SpJWWKPjPNhhULKvD7/c28\ngTgRpZSUKXoAeGhRJWwWCx5/+4TuKERE4yalir4g24l75k7Cs7WncNYb0B2HiGhcpFTRA8DqG6cg\nEI5iHadFIKIUkXJFX1WQhduuLMC6rfXoC0Z0xyEiGnMpV/QA8MhNU9HpC+G5XZwWgYjMLyWLvqZi\nAuaW5+Dxt08gHOEdqIjI3FKy6EUEj9w4FQ0dPt6BiohMLyWLHohNizA5LwO/+PNxKMXJzojIvFK2\n6K0WwedvmIL3mjzYevys7jhERGMmZYseAD55TQnyMh2cFoGITE1r0YvIXSKyxuPRc09Xp92KVQsr\n8afD7TjU0q0lAxHRWNNa9Eqp15RSq91ut7YMn11YgXS7le/qici0UnroBgByXA58+toyvLr3NJo9\nfbrjEBGNupQvegB4ePFkKABPbuFkZ0RkPix6AGW5LnxsVjGe3nEK3f6Q7jhERKOKRR+3+sYp8AbC\n2LC9QXcUIqJRxaKPqy5xY3FVHp7ccgKBMCc7IyLzYNEP8MhNU9DWE8Are0/rjkJENGpY9AMsrsrD\nzOJsrNnMaRGIyDxY9AOICB5ePBl1bV5sPcZpEYjIHFj0F/jY7GJMcNmxfjvvQEVE5sCiv4DTbsWy\nmjJsOtCK1m6/7jhERCPGor+Iz8wvRySq8MwO3oGKiIyPRX8RlXkZuGFaHp7e0cA7UBGR4bHoB7Fi\nQQVauv3430NtuqMQEY0Ii34Qt84oQLHbifXbeFKWiIyNRT8Im9WC5deW4+2jZ1B/pld3HCKiYWPR\nD2H5/DJYLYINOzj/DREZF4t+CIXZTtw+sxDP1p6CP8T5b4jImFj0l7BiQQW6fCH87t1m3VGIiIaF\nRX8Ji6ZOxJS8DH5SlogMi0V/CSKCBxdUYE9DFw6c1nMTcyKikWDRJ+C+a0rhtFuwfhtPyhKR8bDo\nE+B22XHX7El4ZW8TenirQSIyGBZ9glYsqIAvGMFLe5p0RyEiuiws+gRdXZaDWSVurN92kjclISJD\nYdFfhhULynGk1Yud9Z26oxARJYxFfxnuunoSspw2PMX5b4jIQFj0l8HlsOFT15Ri4/5mtPcEdMch\nIkoIi/4yrVhQjlBE4dla3pSEiIyBRX+ZqgqysGBKLjZsb0AkypOyRJT8WPTDsGJBBZq6+vDnI7wp\nCRElPxb9MNw+swh5mWn8pCwRGQKLfhgcNguWX1uGtw634VSHT3ccIqIhseiH6YHryiEAfrOTJ2WJ\nKLmx6IepJCcdN0zLxwu7G3lSloiSGot+BO6bV4pmjx/vHDujOwoR0aBY9COwZGYhsp02PL+rUXcU\nIqJBsehHwGm34hNzJmHj/hZ4+jh9MRElJxb9CC2bV4ZAOMp7yhJR0mLRj9DsUjemFWTiuV28+oaI\nktOoF72IZIjIWhH5pYg8ONp/P9mICJbVlGJPQxfq2ry64xARfUhCRS8iT4pIm4jsv2D5UhE5LCJ1\nIvIP8cWfBPC8UurzAD4xynmT0j1zSmC1CF7YzZOyRJR8En1H/ysASwcuEBErgJ8BuBPATAAPiMhM\nAKUA+scxIqMTM7kVZDtx0/R8vMhr6okoCSVU9EqpzQA6Llg8H0CdUuq4UioI4BkAdwNoRKzsE/77\nZrBsXilauwN4+2i77ihERB8wkiIuwfl37kCs4EsAvAjgUyLycwCvDfbLIrJaRGpFpLa93fjleMuV\nBchx2fEcr6knoiRjG8HvykWWKaVUL4CHLvXLSqk1ANYAQE1NjeHHO9JsVtwzpwQbtjfA4wvB7bLr\njkREBGBk7+gbAZQNeF4K4PTI4hjbffNKEYxE8eq+Jt1RiIjOGUnR7wQwTUQmi4gDwHIAr45OLGO6\nalI2ZhRlcUoEIkoqiV5e+TSArQCuEJFGEXlYKRUG8FcANgE4COBZpdSBsYua/EQE980rxb5GD460\n9uiOQ0QEIPGrbh5QShUrpexKqVKl1BPx5a8rpaYrpaYqpb49tlGN4Z65JbBZhO/qiShppMzlj+Ml\nLzMNH5lRgBd3NyEcieqOQ0Skt+hF5C4RWePxeHTGGHX3zSvFGW8Afz5i/MtGicj4tBa9Uuo1pdRq\nt9utM8aou2VGASZmODh8Q0RJgUM3Y8ButeDuOSX448FWdPYGdcchohTHoh8jy2pKEYoovLKX19QT\nkV4s+jFyZXE2rpqUjec5oyURacaiH0P3zSvF/qZuHGzu1h2FiFIYi34M3T2nBHYrr6knIr14eeUY\nys1w4NYZhXh5TxNCvKaeiDTh5ZVjbFlNKc72BvHWoTbdUYgoRXHoZozdOD0feZlpHL4hIm1Y9GPM\nbrXgk9eU4M1DbWjt9uuOQ0QpiEU/Dj4zvxwRpfDr7Q26oxBRCmLRj4PKvAzcPD0fG7Y3IBjmSVki\nGl8s+nGyalElzngDeP29Zt1RiCjFsOjHyY3T8jE5LwO/eqdedxQiSjEs+nFisQhWLqzA3lNd2Heq\nS3ccIkoh/MDUOLpvXikyHFas3VqvOwoRpRB+YGocZTnt+NS8Uvx2XzPOeAO64xBRiuDQzThbubAC\nwUgUz+zgpZZEND5Y9OOsqiALi6vysH5bA+e/IaJxwaLXYNWiSrR0+/HGgVbdUYgoBbDoNbhlRgFK\nJ6TzpCwRjQsWvQbW+KWWO0508KYkRDTmWPSa3F9TBqfdgrX8ABURjTEWvSY5LgfumVOCl/c2ocsX\n1B2HiEyMRa/RqkWV8Iei+M3OU7qjEJGJ8ZOxGl1ZnI35k3Px1LaTiESV7jhEZFL8ZKxmn1tUicbO\nPrzJWw0S0Rjh0I1mt88sRLHbyZOyRDRmWPSa2awWPHhdObbUnUFdW4/uOERkQiz6JLB8fjkcVgvW\nvnNSdxQiMiEWfRLIy0zDx68uxgu7G9HtD+mOQ0Qmw6JPEp9bVAlfMIIXdjXqjkJEJsOiTxKzS3Mw\ntzwH67aeRJSXWhLRKGLRJ5FVCytx4kwv/ny0XXcUIjIRFn0S+eisYhS7nfjxH49CKb6rJ6LRwaJP\nIg6bBV+6bRr2nurCG+9zrnoiGh2cAiHJfOqaUkzJz8D3Nh1GmHegIqJRwCkQkozNasFXbr8CdW1e\nvLinSXccIjIBDt0koaXVRbi61I0f/eEI/KGI7jhEZHAs+iQkIvjq0hk47fFj/TZ+WpaIRoZFn6QW\nVeXhhml5+Nlbdfy0LBGNCIs+if39HTPQ6Qvh8c3HdUchIgNj0SexWaVufGx2MR7fcgLtPQHdcYjI\noFj0Se7LS6YjEI7iv948qjsKERkUiz7JTcnPxKevLcOGHQ1oOOvTHYeIDIhFbwCP3ToNFhH88I9H\ndEchIgNi0RtAYbYTD10/GS/vbcLB5m7dcYjIYFj0BvHFm6YiK82G7206rDsKERkMi94g3C47vnhz\nFd481IYdJzp0xyEiA2HRG8jnFlWiMDsN/77xEKcxJqKEsegNJN1hxWO3Tseuk53434NtuuMQkUFw\nmmKDWVZTisl5sWmMI7zlIBElgNMUG4zdasGXb5+Ow609eGUvpzEmokvj0I0BfbS6GLNK3PjepsNo\n6/brjkNESY5Fb0AWi+Db91bD0xfCg49vx1kv58EhosGx6A1qdmkOnlh1LRo6fFj55A54+jiVMRFd\nHIvewBZOnYhffHYejrT24KH/2YHeQFh3JCJKQix6g7v5igL89IFrsK/Rg79YW8tbDxLRh7DoTWBp\ndRF+cP/V2HbiLL6wfhcCYZY9EZ3HojeJu+eU4Lv3zsKfDrfjsaf3IhyJ6o5EREmCRW8iy+eX4xt3\nzcTGAy34u+f28QNVRAQAsOkOQKProesnwxeM4HubDiPdYcV37p0FEdEdi4g0YtGb0KMfqUJfMIL/\neqsOTrsV//zxmSx7ohTGojepL98+Hb5gBE/+3wm4HFZ85Y4ZuiMRkSYsepMSEXz941eiLxTBz946\nhp31nfhodRGWVhejyO3UHY+IxpEkw7zmNTU1qra2VncMU4pGFX6x+The2tOII61eAMDc8hzcWV2E\nO6uLUZbr0pyQiIZLRHYppWou+ToWfeqoa/Ni4/5m/H5/Cw6cjt17trokG3dWF2NpdRGm5mdqTkhE\nl4NFT0NqOOvDxgOx0t/T0AUAmF6YieXXluOh6yt58pbIABIteo7Rp6jyiS6svnEqVt84Fc2ePmzc\n34LX9p3Gt377PmxWwcqFlbojEtEo4R2mCMXudDx0/WQ8/4VFuHVGAb712vvYfvys7lhENEp4hyk6\nx2IR/HD5HJTnuvCXv96N0119uiMR0SjgFAj0AdlOO9asnIdAOIovrN/F2TCJTIBFTx9SVZCFH356\nDt5t9OCfXtqPZDhhT0TDx6Kni1oysxBfum0aXtjdiF+9U687DhGNAIueBvU3t0zDkpmF+NffHcTW\nYzw5S2RULHoalMUi+MH9V6NyoguPbtiNxk6f7khENAwsehpSltOONStrEApH8chTu9AX5MlZIqNh\n0dMlTc3PxI8fmIP3m7vxtRff5clZIoNh0VNCbplRiL+9bTpe3nsaT2w5oTsOEV0GFj0l7NGPVGHp\nVUX4zusH8X91Z3THIaIEsegpYRaL4Pv3X42p+Zl4dMNuHGv36o5ERAlg0dNlyUyz4ZcrY5PlfeKn\nW/Bc7SmO2RMlORY9XbbKvAz87m9uQHWJG195/l08umE3unxB3bGIaBAsehqWkpx0bPj8Anx16Qy8\ncaAVS3/0Nt7huD1RUmLR07BZLYIv3jwVL/3l9XClWfHgE9vxndcPIhDmtfZEyYRFTyM2q9SN3/71\nYnxmfjnWbD6Oe3/2DuraenTHIqI4Fj2NCpfDhm/fOwu/XFmDlm4/PvaTLXhqaz1P1BIlARY9jaol\nMwux8Us34LopE/H1Vw7g4bW1aO8J6I5FlNJ4c3AaE9Gowtqt9fju7w/BZhHcfEU+7riqCB+ZUYBs\np113PCJT4M3BSSuLRfDQ9ZOxuCoPv3qnHm+834rX32uB3SpYODUPd1xViCUzC1GQ5dQdlcj0+I6e\nxkU0qrDnVCc2HWjFpgMtOHnWBxFgblkO7riqCHdcVYTKvAzdMYkMJdF39Cx6GndKKRxu7cGm/bHS\nf7+5GwBwRWEW5pbnoKogE1MLMjGtIBOT3OmwWERzYqLkxKInwzjV4cOmAy1481AbDrX0oKP3/Kds\n0+1WTC3IQFV+JqoKzn9VTMyA3cprCSi1sejJsDp6g6hr857/avfiWJsXTV19514jAhRkpaHInY7i\nbCeK3E5MynHGnrudKMp2ojDbCYeNOwMyL56MJcPKzXBg/uRczJ+c+4HlvYEwjrXHyr/+rA8tnj40\ne/w41u7Flroz8AbCH3i9CJCXmYZJbieK3ekoznGiJCf9A4/zM9M4NESmp7XoReQuAHdVVVXpjEEG\nkZFmw+zSHMwuzbnoz3v8IbR4/Djt8Z/bCTR3+XHa04ejbT3YfLQdvgtuhWizCAqzY6Vf5I4dGcSO\nFGJHBIVZThRkp8Fpt47HKhKNCQ7dUMpQSqG7L4ymrj40e/pw2uPH6a4+NHfFHjd7+tDaHUAwHP3Q\n7+a47CjMcqLQ7URhVhpyMx3IdTkwweXAhAwHJrjs8e8OuNPtsPIogcYBh26ILiAicLvscLvsmDkp\n+6KvUUrB0xdCa3cALd1+tHb70dbtjz8OoK3bj8Mt3ejsDSEY+fAOIfbvADnpdkxwOZDjssOdHvvK\ncTmQnT7geXosS//zCS4HzynQmGDREw0gIshxOZDjcuCKoqxBX6eUQm8wgs7eIDp9QXT6Qucf98ae\nd/iC6PIF0e4NoK7dC48vhJ5AGEMdRGel2WJHCxkOTMyIfZ9w7nHa+WUuByZk2JGZZoMIjx5oaCx6\nomEQEWSm2ZCZZkNZrivh34tEFXr8IXj6zn91+ULo6guhqzeIs71BdMS/mrr8eK/Jg47eIEKRi+8d\n7NbYjik3Xvz9Q0m58aOJ7HQ7stJsyHLakemM5c122pDptCHdbuVOIkWw6InGkdVy/oghUUop9ATC\n6OzfEXj7jyKC6OgNocsX2zF0+oI42uY9d2QRvcTpN6vl/M4qO90eO88Q30F84NzDwGUuBzKdNp6D\nMBgWPVGSExFkO+3IdtpRMTGxaSKiUYVufwg9/jB6/GF4A2F4Axc894fR448NJ3X3hdDpC+FgS3fs\nCOMSOwqXwxrbScSPEs59DXiekfbhxxlp1nPP+5dxpzH2WPREJmQZxpHDQNGoQo8/HDtqiJ9r6OwN\nodMXPLeTiO08zu80Gnp96PGH0RuM7UwilzqkiEu3W5HptCFrqB3HgJ9npdmR5YwNR8W+x15j4yel\nB8WiJ6IPsVjOX6FUicufbE4phUA4Cm8gjN5ArPh7A7GdgDcQiT0esJPo3zlcuNPo35EkstNwOawf\n2AE4bVY47RY47db4lwVpNivS7Jb4z6xIs1ngclhjRxcX7GT6jzjMcCUUi56IRp2InCvYvMy0Ef2t\n/p1GT/9QU3z4qX/Y6YPLQ+d2EH3BCLyBMPyhCPzhCAKhaPxx9KKflRiMw2pBpjM27OSy2+B0WJFu\ntyDdbkW6I7aOLoc19txuhdNhhctuHXDEcf7Io//xeM/TxKInoqQ2cKeRnzWynUa/aFQhGIkVvy8Y\nOX90ce5IIwKvP4TeYOTc0Uj/zqMvFPs62xtEX2fscf/f6QtFhrx8tp/TbjlX+l+780osmVk4Kus1\nGBY9EaUci0XgtMR2HjmJXx17Sf1HH75g5NzRRf9Jce/AI49A7HG3Pwx3+tjfcY1FT0Q0SgYefeRm\nDO9E+Fgw/lkGIiIaEoueiMjkWPRERCbHoiciMjkWPRGRybHoiYhMjkVPRGRyLHoiIpNLinvGikg7\ngJPD/PU8AGdGMU4yMNs6mW19APOtk9nWBzDfOl1sfSqUUvmX+sWkKPqREJHaRG6OayRmWyezrQ9g\nvnUy2/oA5lunkawPh26IiEyORU9EZHJmKPo1ugOMAbOtk9nWBzDfOpltfQDzrdOw18fwY/RERDQ0\nM7yjJyKiIRi66EVkqYgcFpE6EfkH3XlGSkTqReQ9EdkrIrW68wyHiDwpIm0isn/AslwR+YOIHI1/\nn6Az4+UYZH2+KSJN8e20V0Q+qjPj5RKRMhF5S0QOisgBEXksvtyQ22mI9THsdhIRp4jsEJF98XX6\nl/jyySKyPb6NfiMiCU16b9ihGxGxAjgCYAmARgA7ATyglHpfa7AREJF6ADVKKcNe+ysiNwLwAlin\nlKqOL/sPAB1KqX+L75AnKKW+qjNnogZZn28C8Cqlvq8z23CJSDGAYqXUbhHJArALwD0APgcDbqch\n1ud+GHQ7iYgAyFBKeUXEDmALgMcA/C2AF5VSz4jIfwPYp5T6+aX+npHf0c8HUKeUOq6UCgJ4BsDd\nmjOlPKXUZgAdFyy+G8Da+OO1iP0nNIRB1sfQlFLNSqnd8cc9AA4CKIFBt9MQ62NYKsYbf2qPfykA\ntwB4Pr484W1k5KIvAXBqwPNGGHzjIrYh3xCRXSKyWneYUVSolGoGYv8pARRozjMa/kpE3o0P7Rhi\niONiRKQSwFwA22GC7XTB+gAG3k4iYhWRvQDaAPwBwDEAXUqpcPwlCXeekYteLrLMmONQ512vlLoG\nwJ0AHo0PG1Dy+TmAqQDmAGgG8J964wyPiGQCeAHAl5RS3brzjNRF1sfQ20kpFVFKzQFQitgIxpUX\ne1kif8vIRd8IoGzA81IApzVlGRVKqdPx720AXkJs45pBa3wctX88tU1znhFRSrXG/xNGAfwSBtxO\n8XHfFwD8Win1YnyxYbfTxdbHDNsJAJRSXQD+BGABgBwRscV/lHDnGbnodwKYFj8L7QCwHMCrmjMN\nm4hkxE8kQUQyANwOYP/Qv2UYrwJYFX+8CsArGrOMWH8Zxt0Lg22n+Im+JwAcVEr9YMCPDLmdBlsf\nI28nEckXkZz443QAtyF27uEtAPfFX5bwNjLsVTcAEL9c6kcArACeVEp9W3OkYRORKYi9iwcAG4AN\nRlwfEXkawM2IzbTXCuAbAF4G8CyAcgANAJYppQxxgnOQ9bkZseEABaAewCP9Y9tGICKLAbwN4D0A\n0fjif0RsXNtw22mI9XkABt1OIjIbsZOtVsTekD+rlPpWvCeeAZALYA+AFUqpwCX/npGLnoiILs3I\nQzdERJQAFj0Rkcmx6ImITI5FT0Rkcix6IiKTY9ETEZkci56IyORY9EREJvf/aTbUa7G135QAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6d4b06a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = np.load('losses.npy')\n",
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to download the vocab, because the tokenization is done differently there than on my laptop (don't know why).\n",
    "with open('vocab.pic', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "id_word = [v[0] for v in vocab]\n",
    "word_id = {w:i for i,w in enumerate(id_word)}\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/coco-caption.ckp-final\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model = RNNCaptioningModel(embedding_size=embeding_size, image_descriptor_size=image_descriptor_size, lstm_size=lstm_size, num_steps=num_steps, vocab_size=vocab_size, sess=sess, checkpoint_path=checkpoint_path)\n",
    "model.build(model_type='infer')\n",
    "model.saver.restore(sess, checkpoint_path + '-final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.90s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "coco = COCO(ms_coco_dir + '/annotations/captions_train2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED: A kitchen is shown with a variety of items on the counters .\n",
      "TRUE: A kitchen is shown with a variety of items on the counters.\n",
      "TRUE: A kitchen has the windows open and plaid curtains.\n",
      "TRUE: A kitchen with two windows and two metal sinks.\n",
      "TRUE: An older kitchen with cluttered counter tops but empty sink.\n",
      "TRUE: Glasses and bottles are placed near a kitchen sink.\n",
      "\n",
      "PRED: A kitchen with wood floors and lots of furniture .\n",
      "TRUE: A kitchen with wood floors and lots of furniture.\n",
      "TRUE: A beautiful, open kitchen and dining room area features an island in the center and wood cabinets and large windows.\n",
      "TRUE: A kitchen made of mostly wood with a small desk with a laptop.\n",
      "TRUE: A very spacious room with a kitchen and dining area.\n",
      "TRUE: A full view of an open kitchen and dining area.\n",
      "\n",
      "PRED: Two cooks are near the stove in a stainless steel kitchen .\n",
      "TRUE: Two chefs in a restaurant kitchen preparing food. \n",
      "TRUE: Two cooks are cooking the food someone ordered at this restaurant\n",
      "TRUE: The chef is cooking with pans on the stove next to an oven. \n",
      "TRUE: Two men that are standing in a kitchen.\n",
      "TRUE: Two cooks are near the stove in a stainless steel kitchen.\n",
      "\n",
      "PRED: Doorway view of a kitchen with a sink , stove , refrigerator and pantry .\n",
      "TRUE: A narrow kitchen filled with appliances and cooking utensils.\n",
      "TRUE: A galley kitchen with cabinets and appliances on both sides\n",
      "TRUE: A hallway leading into a white kitchen with appliances.\n",
      "TRUE: Doorway view of a kitchen with a sink, stove, refrigerator and pantry.\n",
      "TRUE: The pantry door of the small kitchen is closed.\n",
      "\n",
      "PRED: A kitchen has all stainless steel appliances and counters .\n",
      "TRUE: A commercial stainless kitchen with a pot of food cooking. \n",
      "TRUE: Some food sits in a pot in a kitchen. \n",
      "TRUE: A kitchen has all stainless steel appliances and counters.\n",
      "TRUE: a kitchen with a sink and many cooking machines and a pot of food\n",
      "TRUE: Food cooks in a pot on a stove in a kitchen.\n",
      "\n",
      "PRED: Children sitting at computer stations on a long city\n",
      "TRUE: a boy wearing headphones using one computer in a long row of computers\n",
      "TRUE: A little boy with earphones on listening to something.\n",
      "TRUE: A group of people sitting at desk using computers.\n",
      "TRUE: Children sitting at computer stations on a long table.\n",
      "TRUE: A small child wearing headphones plays on the computer.\n",
      "\n",
      "PRED: A professional kitchen and dining area .\n",
      "TRUE: A kitchen with wood floors and lots of furniture.\n",
      "TRUE: A beautiful, open kitchen and dining room area features an island in the center and wood cabinets and large windows.\n",
      "TRUE: A kitchen made of mostly wood with a small desk with a laptop.\n",
      "TRUE: A very spacious room with a kitchen and dining area.\n",
      "TRUE: A full view of an open kitchen and dining area.\n",
      "\n",
      "PRED: A kitchen with cabinets , a stove , microwave and refrigerator .\n",
      "TRUE: A kitchen with a stove, microwave and refrigerator.\n",
      "TRUE: A refrigerator, oven and microwave sitting in a kitchen.\n",
      "TRUE: The kitchenette uses  small space to great efficiency.\n",
      "TRUE: an image of a kitchen setting with black appliances\n",
      "TRUE: A kitchen with cabinets, a stove, microwave and refrigerator.\n",
      "\n",
      "PRED: A woman wearing a net on her head cutting a cake .\n",
      "TRUE: A woman wearing a net on her head cutting a cake. \n",
      "TRUE: A woman cutting a large white sheet cake.\n",
      "TRUE: A woman wearing a hair net cutting a large sheet cake.\n",
      "TRUE: there is a woman that is cutting a white cake\n",
      "TRUE: A woman marking a cake with the back of a chef's knife. \n",
      "\n",
      "PRED: A young girl inhales with the intent of blowing out a candle .\n",
      "TRUE: A young girl inhales with the intent of blowing out a candle. \n",
      "TRUE: A young girl is preparing to blow out her candle.\n",
      "TRUE: A kid is to blow out the single candle in a bowl of birthday goodness. \n",
      "TRUE: Girl blowing out the candle on an ice-cream \n",
      "TRUE: A little girl is getting ready to blow out a candle on a small dessert.\n",
      "\n",
      "PRED: A narrow kitchen mostly with appliances .\n",
      "TRUE: A narrow kitchen filled with appliances and cooking utensils.\n",
      "TRUE: A galley kitchen with cabinets and appliances on both sides\n",
      "TRUE: A hallway leading into a white kitchen with appliances.\n",
      "TRUE: Doorway view of a kitchen with a sink, stove, refrigerator and pantry.\n",
      "TRUE: The pantry door of the small kitchen is closed.\n",
      "\n",
      "PRED: A kitchen with wood floors and lots of furniture .\n",
      "TRUE: A kitchen with wood floors and lots of furniture.\n",
      "TRUE: A beautiful, open kitchen and dining room area features an island in the center and wood cabinets and large windows.\n",
      "TRUE: A kitchen made of mostly wood with a small desk with a laptop.\n",
      "TRUE: A very spacious room with a kitchen and dining area.\n",
      "TRUE: A full view of an open kitchen and dining area.\n",
      "\n",
      "PRED: A kitchen with a stove , microwave and refrigerator .\n",
      "TRUE: A kitchen with a stove, microwave and refrigerator.\n",
      "TRUE: A refrigerator, oven and microwave sitting in a kitchen.\n",
      "TRUE: The kitchenette uses  small space to great efficiency.\n",
      "TRUE: an image of a kitchen setting with black appliances\n",
      "TRUE: A kitchen with cabinets, a stove, microwave and refrigerator.\n",
      "\n",
      "PRED: a guy that is riding his bike next to a train\n",
      "TRUE: A man on a bicycle riding next to a train\n",
      "TRUE: A person is riding a bicycle but there is a train in the background.\n",
      "TRUE: a red and white train and a man riding a bicycle\n",
      "TRUE: a guy that is riding his bike next to a train\n",
      "TRUE: A man riding a bike past a train traveling along tracks.\n",
      "\n",
      "PRED: A woman is holding a cat in her kitchen .\n",
      "TRUE: A woman in a room with a cat.\n",
      "TRUE: A girl smiles as she holds a cat and wears a brightly colored skirt.\n",
      "TRUE: a woman is holding a cat in her kitchen\n",
      "TRUE: A woman is working in a kitchen carrying a soft toy.\n",
      "TRUE: A woman is holding a cat in her kitchen.\n",
      "\n",
      "PRED: A galley kitchen with two sink and cooking machines both pot of food\n",
      "TRUE: A narrow kitchen filled with appliances and cooking utensils.\n",
      "TRUE: A galley kitchen with cabinets and appliances on both sides\n",
      "TRUE: A hallway leading into a white kitchen with appliances.\n",
      "TRUE: Doorway view of a kitchen with a sink, stove, refrigerator and pantry.\n",
      "TRUE: The pantry door of the small kitchen is closed.\n",
      "\n",
      "PRED: A commercial dish wearing in a kitchen with a sink .\n",
      "TRUE: A commercial stainless kitchen with a pot of food cooking. \n",
      "TRUE: Some food sits in a pot in a kitchen. \n",
      "TRUE: A kitchen has all stainless steel appliances and counters.\n",
      "TRUE: a kitchen with a sink and many cooking machines and a pot of food\n",
      "TRUE: Food cooks in a pot on a stove in a kitchen.\n",
      "\n",
      "PRED: Some food in a kitchen front of a stove .\n",
      "TRUE: A commercial stainless kitchen with a pot of food cooking. \n",
      "TRUE: Some food sits in a pot in a kitchen. \n",
      "TRUE: A kitchen has all stainless steel appliances and counters.\n",
      "TRUE: a kitchen with a sink and many cooking machines and a pot of food\n",
      "TRUE: Food cooks in a pot on a stove in a kitchen.\n",
      "\n",
      "PRED: A skateboarder flipping his board on a street .\n",
      "TRUE: A boy performing a kickflip on his skateboard on a city street.\n",
      "TRUE: A man is doing a trick on a skateboard\n",
      "TRUE: A guy jumps in the air with his skateboard beneath him.\n",
      "TRUE: Man in all black doing a trick on his skateboard.\n",
      "TRUE: A skateboarder flipping his board on a street.\n",
      "\n",
      "PRED: An older kitchen with cluttered counter tops but empty sink .\n",
      "TRUE: A kitchen is shown with a variety of items on the counters.\n",
      "TRUE: A kitchen has the windows open and plaid curtains.\n",
      "TRUE: A kitchen with two windows and two metal sinks.\n",
      "TRUE: An older kitchen with cluttered counter tops but empty sink.\n",
      "TRUE: Glasses and bottles are placed near a kitchen sink.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 20\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    idx = np.random.choice(X_captions.shape[0])\n",
    "    \n",
    "    seq = model.infer(X_images[idx].reshape(1,-1), start_id=word_id['START'], end_id=word_id['END'])\n",
    "    print('PRED:', ' '.join([id_word[w] for w in seq if id_word[w] not in ('START', 'END')]))\n",
    "    \n",
    "    imgId = caption_ids[idx]\n",
    "    annIds = coco.getAnnIds(imgIds=[imgId])\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    for a in anns:\n",
    "        print('TRUE:', a['caption'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pisa-txa]",
   "language": "python",
   "name": "conda-env-pisa-txa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
