{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    " * better batching mechanism (maybe the prebuilt in TF)\n",
    " * try different optimizer (SGD with momentum, RMSProp) ?\n",
    " * learning rate decay ?\n",
    " * dropout, more layers, gradient clipping, bidirectional, ... ?\n",
    " * beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vocabulary and a training set\n",
    "\n",
    "I load the Sherlock holmes corpus and process it as follows:\n",
    " * All is converted to lowercase, all whitespace collapsed to a single space\n",
    " * nltk is used to tokenize into sentences and words. Special tokens START and END are added to the beginning and end of each sentence\n",
    " * Only the first 500 sentences are used for training (for simplicity and speed of learning)\n",
    " * All words are kept. Later we may drop all words frequent less than a threshold, or replace them with UNKNOWN token\n",
    " * Longer sentences are trimmed to length of 50 words, shorter sentences are padded (and masking is used in the training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../txa-hw/hw2/pg1661.txt', encoding='utf-8') as f:\n",
    "    original_text = f.read()\n",
    "# Strip meta info and table of contents at the beginning and licence at the end -> use only the book itself.\n",
    "text = original_text[re.search('ADVENTURE I', original_text).start() : re.search('End of the Project Gutenberg EBook', original_text).start()]\n",
    "text = text.lower().strip()\n",
    "text = re.sub('\\s+', ' ', text) # replace whitespaces with single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "token_start = 'START'\n",
    "token_end = 'END'\n",
    "tok_sentences = [[token_start] + [w for w in nltk.word_tokenize(s) if w] + [token_end] for s in sentences]\n",
    "\n",
    "train_size = 100  # limit training set to this number of first sentences\n",
    "tok_sentences = tok_sentences[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(itertools.chain(*tok_sentences))\n",
    "\n",
    "freq_threshold = 0\n",
    "vocab = [(k,v) for k,v in freq_dist.items() if v >= freq_threshold]\n",
    "vocab_size = len(vocab)\n",
    "id_word = [v[0] for v in vocab]\n",
    "word_id = {w:i for i,w in enumerate(id_word)}\n",
    "\n",
    "# drop all words not in vocabulary (less frequent than freq_threshold)\n",
    "# maybe to replace them with special token would be better?\n",
    "# now we actually don't drop anything\n",
    "tok_sentences = [[w for w in s if w in word_id] for s in tok_sentences]\n",
    "\n",
    "max_len = 50\n",
    "\n",
    "# cut all sentences to max_len\n",
    "tok_sentences = [s[:max_len] for s in tok_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[word_id[w] for w in s[:-1]] for s in tok_sentences]\n",
    "Y = [[word_id[w] for w in s[1:]] for s in tok_sentences]  # shift-by-1 X, next-word prediction\n",
    "\n",
    "X_lens = np.asarray([len(x) for x in X])\n",
    "# Y_lens = [len(y) for y in Y]\n",
    "\n",
    "# pad with zeros to max_len\n",
    "X = np.asarray([np.pad(x, (0,max_len-len(x)), 'constant') for x in X])\n",
    "Y = np.asarray([np.pad(y, (0,max_len-len(y)), 'constant') for y in Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reload the module and reimport in case of change in code\n",
    "import importlib\n",
    "import lstm_language_model\n",
    "importlib.reload(lstm_language_model)\n",
    "from lstm_language_model import RNNLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = max_len  # max. number of timesteps\n",
    "embeding_size = 200\n",
    "input_size = X.shape[0]\n",
    "batch_size = 25\n",
    "lstm_size = 200 # n_hidden and state_size?\n",
    "learning_rate = 0.005\n",
    "checkpoint_path = 'checkpoints/sherlock.ckp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "model = RNNLanguageModel(learning_rate=learning_rate, embedding_size=embeding_size, lstm_size=lstm_size, num_steps=num_steps, vocab_size=vocab_size, sess=sess, checkpoint_path=checkpoint_path)\n",
    "model.build(model_type='train')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "epoch 2\n",
      "training loss after 5 steps: 3.4260020256 elapsed time: 00h 00m 02s\n",
      "epoch 3\n",
      "training loss after 10 steps: 1.82269477844 elapsed time: 00h 00m 04s\n",
      "epoch 4\n",
      "training loss after 15 steps: 0.885439217091 elapsed time: 00h 00m 06s\n",
      "epoch 5\n",
      "training loss after 20 steps: 0.464388102293 elapsed time: 00h 00m 08s\n",
      "saved model to checkpoints/sherlock.ckp-20\n",
      "epoch 6\n",
      "epoch 7\n",
      "training loss after 25 steps: 0.338384181261 elapsed time: 00h 00m 11s\n",
      "epoch 8\n",
      "training loss after 30 steps: 0.309390157461 elapsed time: 00h 00m 13s\n",
      "epoch 9\n",
      "training loss after 35 steps: 0.292283564806 elapsed time: 00h 00m 15s\n",
      "epoch 10\n",
      "training loss after 40 steps: 0.288000762463 elapsed time: 00h 00m 17s\n",
      "saved model to checkpoints/sherlock.ckp-40\n",
      "epoch 11\n",
      "epoch 12\n",
      "training loss after 45 steps: 0.280894309282 elapsed time: 00h 00m 21s\n",
      "epoch 13\n",
      "training loss after 50 steps: 0.272980362177 elapsed time: 00h 00m 23s\n",
      "epoch 14\n",
      "training loss after 55 steps: 0.283316373825 elapsed time: 00h 00m 24s\n",
      "epoch 15\n",
      "training loss after 60 steps: 0.268651634455 elapsed time: 00h 00m 26s\n",
      "saved model to checkpoints/sherlock.ckp-60\n",
      "epoch 16\n",
      "epoch 17\n",
      "training loss after 65 steps: 0.263331741095 elapsed time: 00h 00m 30s\n",
      "epoch 18\n",
      "training loss after 70 steps: 0.270344823599 elapsed time: 00h 00m 32s\n",
      "epoch 19\n",
      "training loss after 75 steps: 0.267868936062 elapsed time: 00h 00m 34s\n",
      "epoch 20\n",
      "training loss after 80 steps: 0.258104592562 elapsed time: 00h 00m 36s\n",
      "saved model to checkpoints/sherlock.ckp-80\n",
      "Finished training\n",
      "Saved final model to checkpoints/sherlock.ckp-final\n",
      "Final training loss: 0.258104592562\n",
      "It took 00h 00m 39s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "model.train(X, Y, X_lens, n_epochs, batch_size, evaluate_every=5, save_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x171f44a8>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9CIiIaeiFxEJORW9iEjIqehFRELu/wNHxcnD/6rIYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x175de668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogy(model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sherlock.ckp-final\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "model.sess = sess\n",
    "model.build(model_type='infer')\n",
    "model.saver.restore(sess, checkpoint_path + '-final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this account of you we have from all quarters received . END \n",
      "`` i think , watson , that you have put on seven and a half pounds since i saw you . '' END \n",
      "this account of you we have from all quarters received . END \n",
      "i do n't know . '' END \n",
      "'p , ' of course , stands for 'papier . ' END \n",
      "i answered . END \n",
      "you did not tell me that you intended to go into harness . '' END \n",
      "insensibly one begins to twist facts to suit theories , instead of theories to suit facts . END \n",
      "`` i have no data yet . END \n",
      "`` seven ! '' END \n",
      "his manner was not effusive . END \n",
      "as to mary jane , she is incorrigible , and my wife has given her notice , but there , again , i fail to see how you work it out . '' END \n",
      "and in practice again , i observe . END \n",
      "i carefully examined the writing , and the paper upon which it was written . END \n",
      "for example , you have frequently seen the steps which lead up from the hall to this room . '' END \n",
      "`` , then how do you know ? '' END \n",
      "`` how many ? END \n",
      "`` how often ? '' END \n",
      "she is just a trifle more , i fancy , watson . END \n",
      "for example , you have frequently seen the steps which lead up from the hall to this room . '' END \n",
      "`` then how many are there ? '' END \n",
      "`` peculiar -- that is the very word , '' said holmes . END \n",
      "i could not help laughing at the ease with which he explained his process of deduction . END \n",
      "`` i think , watson , that you have put on seven and a half pounds since i saw you . '' END \n",
      "he was still , as ever , deeply attracted by the study of crime , and occupied his immense faculties and extraordinary powers of observation in following out those clues , and clearing up those mysteries which had been abandoned as hopeless by the official police . END \n",
      "my own complete happiness , and the home-centred interests which rise up around the man who first finds himself master of his own establishment , were sufficient to absorb all my attention , while holmes , who loathed every form of society with his whole bohemian soul , remained , and indicated a dark silhouette against the blind . END \n",
      "`` wedlock suits you , '' he answered , lighting a cigarette , and throwing himself down into an armchair . END \n",
      "`` there will call upon you to-night , at a quarter to eight o'clock , '' it said , `` a gentleman who desires to consult you upon a matter of the very deepest moment . END \n",
      "he had risen out of his drug-created dreams and was hot "
     ]
    }
   ],
   "source": [
    "seq = ['START']\n",
    "seq = [word_id[s] for s in seq]\n",
    "state = init_state = np.zeros((2, 1, lstm_size))\n",
    "\n",
    "num_words = 500\n",
    "\n",
    "for i in range(len(seq)-1, num_words):\n",
    "    p, state = model.infer(np.reshape(seq[-1], (1,1)), np.array([1]), state)\n",
    "#     max_id = np.argmax(p)\n",
    "    p = p.reshape([-1])\n",
    "    max_id = np.random.choice(list(range(len(p))), p=p)\n",
    "    seq.append(max_id)\n",
    "    print(id_word[max_id], end=' ')\n",
    "    if max_id == word_id[token_end]: # restart the sentence\n",
    "        print()\n",
    "        seq.append(word_id[token_start])\n",
    "        state = init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pisa-txa]",
   "language": "python",
   "name": "conda-env-pisa-txa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
